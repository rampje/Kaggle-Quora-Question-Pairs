---
title: "Quora Question Pairs"
output: html_notebook
---

This is an R Markdown notebook containing exploratory data analysis (EDA), text mining, and modeling for Kaggle's [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) competition. The goal is to build a model that can accurately classify whether  questions are duplicates. Updated source code available on [Github](https://github.com/rampje/Kaggle-Quora-Question-Pairs/blob/master/exploration.Rmd)

```{r}
library(plyr)
library(tidyverse)
library(purrr)
library(tm)
library(lsa)
library(caret)
library(xgboost)
library(reshape2)
```

```{r}
# Load packages and read the training data into R
train <- read.csv("train.csv", stringsAsFactors = F)
```

Click to hide functions used in script:

```{r}
# Text processing pipeline where input x is either question1 or question2
txt_proc_pipeline <- function(x){
  x %>% 
    tolower %>% 
    tm::removeWords(stopwords("english")) %>%
    strsplit(" ") %>%
    map(tm::removePunctuation)}
    

# Wrapper function to calculate cosine similarity between two question columns
# This is applied to the test data set after being explicitly laid out for the training data set
cosine_similarity <- function(x, y){
  # apply text processing pipeline to inputs
  x <- txt_proc_pipeline(x)
  y <- txt_proc_pipeline(y)
  
  # create list containing vectors of union sets between x and y
  union_set <- Map(c, x, y)
  union_set <- map(union_set, unique)
  
  # initialize empty lists to contain basis vectors for questions as well as vector of cosine similarities
  x_stage <- vector("list", length(union_set))
  y_stage <- vector("list", length(union_set))
  cosine_sims <- numeric()
  # populate lists containing basis vectors for questions as well as vector of cosine similarities
  for(i in seq_along(union_set)){
    x_stage[[i]] <- is.element(union_set[[i]], x[[i]])
    y_stage[[i]] <- is.element(union_set[[i]], y[[i]])
    
    sim <- as.numeric(lsa::cosine(x_stage[[i]],
                                  y_stage[[i]]))
    cosine_sims <- c(cosine_sims, sim)}
  
  cosine_sims}
```

Take a look at the training data set
```{r}
glimpse(train)
```

What do the question pairs look like?
```{r}
head(train[c("question1","question2","is_duplicate")], 10)
```

Get a histogram of question length differences:
```{r}
train$char_diff <- nchar(train$question1) - nchar(train$question2)
```

```{r}
plotStage <- train %>% filter(char_diff > -200 & char_diff < 200)
h <- ggplot(data=plotStage, aes(x=char_diff))
h + geom_histogram(aes(fill = as.character(plotStage$is_duplicate)),
                   binwidth = 5, colour = "black")
```
  


## Calculating cosine similarities   

This blog [**post**](http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/) provides an introduction to the [**Vector Space Model** (**VSM**)](https://en.wikipedia.org/wiki/Vector_space_model) and [**term frequency-inverse document frequency** (**tf-idf**)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). These concepts provide the mathematical foundations which motivate the usage of cosine similarity to determine whether texts are the same.

#### Definition of cosine similarity  

The [cosine similarity](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/) of two vectors **a** and **b** is an idea related to the [dot product](https://en.wikipedia.org/wiki/Dot_product), defined as:

$$\vec{a}\cdot \vec{b} = \sum_{i=1}^{n}a_{i}b_{i}$$

The dot product is a single scalar value resulting summing all the elements of **a** and **b** multiplied together. The cosine similarity can be derived by rewriting the dot product formula:

$$\vec{a}\cdot \vec{b} = \left \| \vec{a} \right \|  \left \| \vec{b} \right \| cos(\Theta)$$

$$ similarity = cos(\Theta) = \frac{\vec{a}\cdot \vec{b}}{\left \| \vec{a} \right \|  \left \| \vec{b} \right \|} $$

Similarity takes a value between 0 and 1.


#### Transforming data
Question columns need to be transformed in order to calculate cosine similarity
```{r}
# make words lowercase
train$question1 <- tolower(train$question1)
train$question2 <- tolower(train$question2)

# turn vectors of questions into list containing vectors of question words
q1 <- strsplit(train$question1, " ")
q2 <- strsplit(train$question2, " ")

# remove punctuation
q1 <- map(q1, tm::removePunctuation)
q2 <- map(q2, tm::removePunctuation)
q1[[1]] # example
```

Set up the lists of vectors that will be inputed into cosine()

```{r}
shell <- Map(c, q1, q2)
shell <- map(shell, unique)
shell[[1]]
```

In order to create the question vectors, 2 lists need to be initialized. These lists are then populated in a for loop with logical vectors indicating set membership of words that appear in the union of question 1 and question 2 words.
```{r}
stage1 <- vector("list", length(shell))
stage2 <- vector("list", length(shell))
for(x in 1:length(shell)){
  stage1[[x]] <- is.element(shell[[x]], q1[[x]])
  stage2[[x]] <- is.element(shell[[x]], q2[[x]])}

# a sample of both vectors
data.frame("q1"=as.numeric(stage1[[21]]), "q2"=as.numeric(stage2[[21]]), "word"=shell[[21]])
```

Build vector containing cosine similarities using a for loop. This took about 5-6 minutes to run

```{r}
cos_sims <- numeric()
for(x in 1:length(shell)){
  sim <- as.numeric(lsa::cosine(stage1[[x]], stage2[[x]]))
  cos_sims <- c(cos_sims, sim)}
head(cos_sims, 20)
```

Distribution of question pairs' cosine similarities 
```{r}
train$cos_sims <- cos_sims
h <- ggplot(data=train, aes(x=cos_sims))
h + geom_histogram(aes(fill = as.character(train$is_duplicate)),
                   binwidth = 0.01, colour = "black")
```


```{r}
s <- ggplot(data = train, aes(char_diff, cos_sims))
s + geom_point(aes(colour = factor(is_duplicate)),
               alpha = 0.2)
```

Add a few more features 

```{r}
train$q1_wc <- map_dbl(q1, length)
train$q2_wc <- map_dbl(q2, length)
train$wc_diff <- train$q1 - train$q2

# also make this a factor now to avoid problems modeling later
train$is_duplicate <- as.factor(train$is_duplicate)

glimpse(train)
```

```{r}
plotStage <- train %>% filter(wc_diff > -30 & wc_diff < 30)
h <- ggplot(data=plotStage, aes(x=wc_diff))
h + geom_histogram(aes(fill = as.character(plotStage$is_duplicate)),
                   binwidth=1, colour = "black")
```

### Highest occurence words

There are some terms that may be worth tagging
```{r}
train_words <- train[c("id","question1","question2")]
train_words <- melt(train_words, id.vars = "id")
train_words <- txt_proc_pipeline(train_words$value)
train_words <- unlist(train_words)
train_words <- tm::removeWords(train_words, stopwords("english"))
head(sort(table(train_words), decreasing=T), 100)
```

Word occurence features found from looking at *train_words*.

```{r}
word_occurences <- c("life","money","indian","trump","2016","engineering","donald","google","facebook","language","sex", "war", "programming", "president", "women","android","iphone","clinton","hillary")
word_occurences
```

Get word occurence flags in a list
```{r}
word_flags <- vector("list", length(word_occurences))

for(x in seq_along(word_occurences)){
  word_flags[[x]] <- grepl(word_occurences[x], train$question1) &
    grepl(word_occurences[x], train$question2)
}
```

```{r}
head(summary(word_flags))
```

```{r}
word_flags <- data.frame(word_flags)
names(word_flags) <- word_occurences
word_flags$id <- train$id
head(word_flags, 10)
```

```{r}
train <- full_join(train, word_flags, by = "id")
glimpse(train)
```


### Fit Some Models

```{r}
test <- read.csv("test.csv", stringsAsFactors = F)
```

#### Prepare test data set for modeling
```{r}
test$char_diff <- nchar(test$question1) - nchar(test$question2)
test$q1_wc <- test$question1 %>% 
                      txt_proc_pipeline %>%
                      map_dbl(length)
test$q2_wc <- test$question2 %>% 
                      txt_proc_pipeline %>%
                      map_dbl(length)
```

```{r}
test_word_flags <- vector("list", length(word_occurences))

for(x in seq_along(word_occurences)){
  test_word_flags[[x]] <- grepl(word_occurences[x], test$question1) &
    grepl(word_occurences[x], test$question2)}

test_word_flags <- data.frame(test_word_flags)
names(test_word_flags) <- word_occurences
test_word_flags$test_id <- test$test_id

test <- full_join(test, test_word_flags)

glimpse(test)
```


May have to figure out a faster way to implement this...

```{r}
t1 <- Sys.time()
test_cos_sims2 <- cosine_similarity(test$question1, test$question2)
t2 <- Sys.time()
t2-t1
```

```{r}
cos_sims_csv <- read.csv("cos_sims_v2.csv")
test$cos_sims <- cos_sims_csv$x
```

```{r}
test$cos_sims <- test_cos_sims2
```

#### These are the records that got NA for cosine similarity calculation
```{r}
test %>% filter(is.na(cos_sims))
```


#### Boosted trees for classification
Using *caret* package's extreme gradient (XG) Boosted trees method. 

  + [XG boosted trees introduction](http://xgboost.readthedocs.io/en/latest/model.html)
  + [Tuning parameters](http://xgboost.readthedocs.io/en/latest//parameter.html)
  + [*caret* train() models](https://topepo.github.io/caret/train-models-by-tag.html)

Fit boosted trees predicting binary target
```{r}
# go to dataframe with response and predictors for tree model
bt_df2 <- train[c("is_duplicate","char_diff","cos_sims","q1_wc","q2_wc",word_occurences)]

set.seed(1) 

# tuning parameters
gbmGrid <- expand.grid(
  nrounds = 10, # number of boosting iterations
  max_depth = 3000, # max tree depth 
  eta = 0.01, # shrinkage / learning rate
  gamma = 0, # minimum loss function
  colsample_bytree = 1, # subsample of ratio of columns
  min_child_weight=1, # minimum sum of instance weight
  subsample = 0.5 # subsample shrinkage
  )

gbmTree2 <- train(is_duplicate ~ ., 
                 data = bt_df2,
                 method = "xgbTree",
                # trControl = fitControl,
                 verbose = FALSE,
                 na.action = na.omit,
                 tuneGrid = gbmGrid)
```

#### Model summary
```{r}
gbmTree2
```

Predict testing data

```{r}
# replace na similarity calculations for predictions to work
test$cos_sims[is.na(test$cos_sims)] <- 1

gbt2_preds <- predict(gbmTree2 , newdata = test, type = "prob")
gbt2_preds$test_id <- test$test_id
head(gbt2_preds)
```

Prepare submission format

```{r}
submission2 <- gbt2_preds[c("test_id","1")]
names(submission2)[2] <- "is_duplicate"
head(submission2)
```

```{r}
write.csv(submission2, "submission2.csv", row.names = FALSE)
```

